{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZS4dp4S51XV",
        "outputId": "df7fe5e1-de89-43ff-9588-698f7c143b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocess"
      ],
      "metadata": {
        "id": "q7DfkgSIAYiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "-89pILMy6BJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Jt6xHUjK6px3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/cmn-eng/eng-cn.pkl\", \"rb\") as f:\n",
        "    seq_pairs = pkl.load(f)"
      ],
      "metadata": {
        "id": "k28h3jYy6KXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences = [pair[0] for pair in seq_pairs[:10000]]    #RAM有限，所以只取數據集中一萬筆數據做訓練\n",
        "tgt_sentences = [pair[1] for pair in seq_pairs[:10000]]"
      ],
      "metadata": {
        "id": "aYt5oMP462fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8enuT3zf6yBh",
        "outputId": "ff894792-2ea1-469d-d723-c4ac0997c18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> hi . <END>',\n",
              " '<START> hi . <END>',\n",
              " '<START> run . <END>',\n",
              " '<START> stop ! <END>',\n",
              " '<START> wait ! <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCmLSDZP6zJJ",
        "outputId": "49c1026d-3a70-4dae-e05d-879e6b455b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> 嗨 。 <END>',\n",
              " '<START> 你 好 。 <END>',\n",
              " '<START> 你 用 跑 的 。 <END>',\n",
              " '<START> 住 手 ！ <END>',\n",
              " '<START> 等 等 ！ <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokeniser(sentences):\n",
        "    # create a tokeniser specific to texts\n",
        "    tokeniser = Tokenizer(filters = ' ')\n",
        "    tokeniser.fit_on_texts(sentences)\n",
        "    # 預覽前 3 個data及label\n",
        "    for i in range(3):\n",
        "        print(\"original: {} - word tokenised: {}\".format(sentences[i], tokeniser.texts_to_sequences(sentences)[i]))\n",
        "\n",
        "    return tokeniser.texts_to_sequences(sentences), tokeniser\n",
        "\n",
        "# word tokenise source and target sentences\n",
        "src_word_tokenised, src_tokeniser = create_tokeniser(src_sentences)\n",
        "tgt_word_tokenised, tgt_tokeniser = create_tokeniser(tgt_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBCQo_Cy69rD",
        "outputId": "ff1e1913-bd1e-4f7b-f24c-9e5ddb6cbc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: <START> hi . <END> - word tokenised: [1, 730, 3, 2]\n",
            "original: <START> hi . <END> - word tokenised: [1, 730, 3, 2]\n",
            "original: <START> run . <END> - word tokenised: [1, 322, 3, 2]\n",
            "original: <START> 嗨 。 <END> - word tokenised: [1, 1284, 3, 2]\n",
            "original: <START> 你 好 。 <END> - word tokenised: [1, 6, 25, 3, 2]\n",
            "original: <START> 你 用 跑 的 。 <END> - word tokenised: [1, 6, 138, 268, 7, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source and target vocabulary dictionaries\n",
        "src_vocab_dict = src_tokeniser.word_index  #每個單詞的token ID\n",
        "tgt_vocab_dict = tgt_tokeniser.word_index\n",
        "\n",
        "src_vocab_size = len(src_vocab_dict) + 1 # 3080 tokens in total\n",
        "tgt_vocab_size = len(tgt_vocab_dict) + 1 # 2455 tokens in total"
      ],
      "metadata": {
        "id": "UnyINdR-9t_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GboOtZP-9wFj",
        "outputId": "5a9246fa-942e-4e98-a1ba-23c24a9ebe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2455"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_max_seq_length = len(max(src_word_tokenised, key = len)) # 11  #數據中最長句子長度\n",
        "tgt_max_seq_length = len(max(tgt_word_tokenised, key = len)) # 22  #標籤中最長句子長度"
      ],
      "metadata": {
        "id": "faxejOaB9yYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_max_seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zED7fgovB4Cw",
        "outputId": "889fa434-737b-4dd6-c220-bd49b9d49f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#讓數據及標籤標準化，長度相等，用0填充\n",
        "src_sentences_padded = pad_sequences(src_word_tokenised, maxlen = src_max_seq_length, padding = \"post\")  # shape: (10000, 11)\n",
        "tgt_sentences_padded = pad_sequences(tgt_word_tokenised, maxlen = tgt_max_seq_length, padding = \"post\")  # shape: (10000, 22)\n",
        "\n",
        "# increase 1 dimension\n",
        "src_sentences_padded = src_sentences_padded.reshape(*src_sentences_padded.shape, 1) # shape: (10000, 11, 1)\n",
        "tgt_sentences_padded = tgt_sentences_padded.reshape(*tgt_sentences_padded.shape, 1) # shape: (10000, 22, 1)"
      ],
      "metadata": {
        "id": "qRFCJOcc-Rfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_sentences_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOErySlW_OH9",
        "outputId": "0d1115c0-02fd-4aed-e6ec-3701033149d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 22, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_sequences(tokeniser, max_seq_length, sentences):\n",
        "    \"\"\"\n",
        "    Label encode every sentences to create features X\n",
        "    \"\"\"\n",
        "    # label encode every sentences\n",
        "    sentences_le = tokeniser.texts_to_sequences(sentences)\n",
        "    # pad sequences with zeros at the end\n",
        "    X = pad_sequences(sentences_le, maxlen = max_seq_length, padding = \"post\")\n",
        "    return X\n",
        "\n",
        "\n",
        "def encode_output_labels(sequences, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encode target sequences to create labels y\n",
        "    \"\"\"\n",
        "    y_list = []\n",
        "    for seq in sequences:\n",
        "        # one-hot encode each sentence\n",
        "        oh_encoded = to_categorical(seq, num_classes = vocab_size)\n",
        "        y_list.append(oh_encoded)\n",
        "    y = np.array(y_list, dtype = np.float32)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y\n",
        "\n",
        "# create encoder inputs, decoder inputs and decoder outputs    #n_samples=10000\n",
        "enc_inputs = encode_input_sequences(src_tokeniser, src_max_seq_length, src_sentences) # shape: (n_samples=10000, src_max_seq_length, 1)\n",
        "dec_inputs = encode_input_sequences(tgt_tokeniser, tgt_max_seq_length, tgt_sentences) # shape: (n_samples, tgt_max_seq_length, 1)\n",
        "dec_outputs = encode_input_sequences(tgt_tokeniser, tgt_max_seq_length, tgt_sentences)\n",
        "dec_outputs = encode_output_labels(dec_outputs, tgt_vocab_size) # shape: (n_samples, tgt_max_seq_length, tgt_vocab_size )"
      ],
      "metadata": {
        "id": "GZp0zcivA--Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_outputs[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUfBnaeFA_f4",
        "outputId": "92b35789-2a9f-4c4c-8c3e-597b30635ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 2455)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save dataset"
      ],
      "metadata": {
        "id": "8eewNjH4GXEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save required data to a compressed file\n",
        "'''\n",
        "np.savez_compressed(\"/content/drive/MyDrive/cmn-eng/eng-cn_data.npz\", enc_inputs = enc_inputs, dec_inputs = dec_inputs, dec_outputs = dec_outputs, src_vocab_size = src_vocab_size)\n",
        "'''"
      ],
      "metadata": {
        "id": "5z-A5f_sBot_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0db28c2c-6aa0-4a3c-d66d-50ba450583ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnp.savez_compressed(\"/content/drive/MyDrive/cmn-eng/eng-cn_data.npz\", enc_inputs = enc_inputs, dec_inputs = dec_inputs, dec_outputs = dec_outputs, src_vocab_size = src_vocab_size)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train data and test data"
      ],
      "metadata": {
        "id": "3aeUXkWHGgK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/drive/MyDrive/cmn-eng/eng-cn_data.npz\")\n",
        "print(data.files) # ['enc_inputs', 'dec_inputs', 'dec_outputs', 'src_vocab_size']\n",
        "\n",
        "# Extract our desired data\n",
        "enc_inputs = data[\"enc_inputs\"]\n",
        "dec_inputs = data[\"dec_inputs\"]\n",
        "dec_outputs = data[\"dec_outputs\"]\n",
        "src_vocab_size = data[\"src_vocab_size\"].item(0)  #type is int 3080"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ng0A_lxGkqO",
        "outputId": "84a8abe6-a49f-40f1-c0b4-e7921eb594db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['enc_inputs', 'dec_inputs', 'dec_outputs', 'src_vocab_size']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle X and y in unision\n",
        "shuffler = np.random.permutation(enc_inputs.shape[0])\n",
        "enc_inputs = enc_inputs[shuffler]\n",
        "dec_inputs = dec_inputs[shuffler]\n",
        "dec_outputs = dec_outputs[shuffler]"
      ],
      "metadata": {
        "id": "d25LxBiWG7aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# prepare training and test data\n",
        "test_ratio = .2\n",
        "enc_inputs_train, enc_inputs_test = train_test_split(enc_inputs, test_size = test_ratio, shuffle = False)\n",
        "dec_inputs_train, dec_inputs_test = train_test_split(dec_inputs, test_size = test_ratio, shuffle = False)\n",
        "y_train, y_test = train_test_split(dec_outputs, test_size = test_ratio, shuffle = False)\n",
        "X_train = [enc_inputs_train, dec_inputs_train]\n",
        "X_test = [enc_inputs_test, dec_inputs_test]"
      ],
      "metadata": {
        "id": "IfkUotZQHAZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Model"
      ],
      "metadata": {
        "id": "IaCzcvsZI1H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Activation, dot, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nMiMT18_Kwt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tabzKVnTLbaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_max_seq_length = enc_inputs.shape[1]\n",
        "tgt_max_seq_length = dec_outputs.shape[1]\n",
        "tgt_vocab_size = dec_outputs.shape[2]"
      ],
      "metadata": {
        "id": "H1jDVpg9HwK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "src_wordEmbed_dim = 96\n",
        "tgt_wordEmbed_dim = 100\n",
        "latent_dim = 256\n",
        "\n",
        "def build_seq2seq(src_max_seq_length, src_vocab_size, src_wordEmbed_dim, tgt_max_seq_length, tgt_vocab_size, tgt_wordEmbed_dim, latent_dim, model_name = None):\n",
        "    \"\"\"\n",
        "    Builda an LSTM seq2seq model with Luong attention\n",
        "    \"\"\"\n",
        "    # Build an encoder\n",
        "    enc_inputs = Input(shape = (src_max_seq_length, ))\n",
        "    vectors = Embedding(input_dim = src_vocab_size, output_dim = src_wordEmbed_dim, name = \"embedding_enc\")(enc_inputs)\n",
        "    enc_outputs_1, enc_h1, enc_c1 = LSTM(latent_dim, return_sequences = True, return_state = True, name = \"1st_layer_enc_LSTM\")(vectors)\n",
        "    enc_outputs_2, enc_h2, enc_c2 = LSTM(latent_dim, return_sequences = True, return_state = True, name = \"2nd_layer_enc_LSTM\")(enc_outputs_1)\n",
        "    enc_states = [enc_h1, enc_c1, enc_h2, enc_c2]\n",
        "\n",
        "    # Build a decoder\n",
        "    dec_inputs = Input(shape = (tgt_max_seq_length, ))\n",
        "    vectors = Embedding(input_dim = tgt_vocab_size, output_dim = tgt_wordEmbed_dim, name = \"embedding_dec\")(dec_inputs)\n",
        "    dec_outputs_1, dec_h1, dec_c1 = LSTM(latent_dim, return_sequences = True, return_state = True, name = \"1st_layer_dec_LSTM\")(vectors, initial_state = [enc_h1, enc_c1])\n",
        "    dec_outputs_2 = LSTM(latent_dim, return_sequences = True, return_state = False, name = \"2nd_layer_dec_LSTM\")(dec_outputs_1, initial_state = [enc_h2, enc_c2])\n",
        "\n",
        "    # evaluate attention score\n",
        "    attention_scores = dot([dec_outputs_2, enc_outputs_2], axes = [2, 2])\n",
        "    attenton_weights = Activation(\"softmax\")(attention_scores)\n",
        "    context_vec = dot([attenton_weights, enc_outputs_2], axes = [2, 1])\n",
        "    ht_context_vec = concatenate([context_vec, dec_outputs_2], name = \"concatentated_vector\")\n",
        "    attention_vec = Dense(latent_dim, use_bias = False, activation = \"tanh\", name = \"attentional_vector\")(ht_context_vec)\n",
        "    logits = TimeDistributed(Dense(tgt_vocab_size))(attention_vec)\n",
        "    dec_outputs_final = Activation(\"softmax\", name = \"softmax\")(logits)\n",
        "\n",
        "    # integrate as a model\n",
        "    model = Model([enc_inputs, dec_inputs], dec_outputs_final, name = model_name)\n",
        "    # compile model\n",
        "    model.compile(\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
        "        loss = tf.keras.losses.CategoricalCrossentropy(),   #metrics=[masked_acc, masked_loss]\n",
        "        metrics = [tf.keras.metrics.CategoricalAccuracy(name='acc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# build our seq2seq model\n",
        "eng_cn_translator = build_seq2seq(\n",
        "    src_max_seq_length = src_max_seq_length,\n",
        "    src_vocab_size = src_vocab_size,\n",
        "    src_wordEmbed_dim = src_wordEmbed_dim,\n",
        "    tgt_max_seq_length = tgt_max_seq_length,\n",
        "    tgt_vocab_size = tgt_vocab_size,\n",
        "    tgt_wordEmbed_dim = tgt_wordEmbed_dim,\n",
        "    latent_dim = latent_dim,\n",
        "    model_name = \"eng-cn_translator_v1\"\n",
        "    )\n",
        "eng_cn_translator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "BpMgSzxkJbxX",
        "outputId": "3b294c16-d46a-40f1-8859-94795dbe7ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"eng-cn_translator_v1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"eng-cn_translator_v1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_enc (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m96\u001b[0m)         │        \u001b[38;5;34m295,680\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_dec (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m245,500\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 1st_layer_enc_LSTM (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m361,472\u001b[0m │ embedding_enc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │                        │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 1st_layer_dec_LSTM (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m365,568\u001b[0m │ embedding_dec[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ 1st_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │ 1st_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 2nd_layer_enc_LSTM (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m525,312\u001b[0m │ 1st_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │                        │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 2nd_layer_dec_LSTM (\u001b[38;5;33mLSTM\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m525,312\u001b[0m │ 1st_layer_dec_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (\u001b[38;5;33mDot\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m11\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ 2nd_layer_dec_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m11\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot_1 (\u001b[38;5;33mDot\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatentated_vector      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dot_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ 2nd_layer_dec_LSTM[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attentional_vector        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m131,072\u001b[0m │ concatentated_vector[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m2455\u001b[0m)       │        \u001b[38;5;34m630,935\u001b[0m │ attentional_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ softmax (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m2455\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ time_distributed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_enc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">295,680</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_dec (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">245,500</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 1st_layer_enc_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">361,472</span> │ embedding_enc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 1st_layer_dec_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ embedding_dec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ 1st_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │ 1st_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 2nd_layer_enc_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ 1st_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ 2nd_layer_dec_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ 1st_layer_dec_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ 2nd_layer_dec_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ 2nd_layer_enc_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatentated_vector      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ 2nd_layer_dec_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attentional_vector        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │ concatentated_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2455</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">630,935</span> │ attentional_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2455</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,080,851\u001b[0m (11.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,080,851</span> (11.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,080,851\u001b[0m (11.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,080,851</span> (11.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict the model"
      ],
      "metadata": {
        "id": "zubnB-_ePTsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# load pre-trained model\n",
        "eng_cn_translator = load_model('/content/drive/MyDrive/Colab Notebooks/eng-cmn project/Save_model/eng-cn_translator_v2.keras')"
      ],
      "metadata": {
        "id": "ZyI2kBIhPTPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict model\n",
        "trans_seqs = eng_cn_translator.predict(\n",
        "                X_test,\n",
        "                batch_size = 60,\n",
        "                verbose = 1,\n",
        "                #use_multiprocessing = True\n",
        "            )\n",
        "print(trans_seqs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tHkELJSQWl5",
        "outputId": "443fc773-2f69-4a3a-c134-c7ea13716475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 249ms/step\n",
            "[[3.2128755e-10 9.9999803e-01 3.4186824e-12 ... 1.4705118e-15\n",
            "  5.8408575e-18 1.6769500e-16]\n",
            " [3.1389467e-11 4.7334123e-11 6.0229860e-10 ... 8.9220291e-14\n",
            "  1.5915183e-14 5.7733267e-12]\n",
            " [3.7189602e-11 4.2853042e-13 3.0316141e-10 ... 4.4653253e-14\n",
            "  6.2684561e-17 2.3180808e-12]\n",
            " ...\n",
            " [9.9999791e-01 2.2203528e-08 5.8465798e-07 ... 5.8586985e-14\n",
            "  6.7402501e-17 1.7426949e-12]\n",
            " [9.9999791e-01 2.2180460e-08 5.8612119e-07 ... 5.8399554e-14\n",
            "  6.7350329e-17 1.7424256e-12]\n",
            " [9.9999791e-01 2.2165869e-08 5.8596862e-07 ... 5.8348779e-14\n",
            "  6.7333369e-17 1.7418043e-12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 zip 函數對調\n",
        "reverse_src_vocab_dict = dict(zip(src_vocab_dict.values(), src_vocab_dict.keys()))\n",
        "reverse_tgt_vocab_dict = dict(zip(tgt_vocab_dict.values(), tgt_vocab_dict.keys()))"
      ],
      "metadata": {
        "id": "eG63-iTccVdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_seq(model, single_seq_pair, reverse_tgt_vocab_dict):\n",
        "    \"\"\"\n",
        "    Predicts a single sentence\n",
        "    ---------------------------\n",
        "    single_seq_pair:\n",
        "        sequence pair that is made up of only one source sequence and one target sequence [(src_max_seq_length, ), (tgt_max_seq_length, )]\n",
        "        type: list of NumPy arrays\n",
        "    \"\"\"\n",
        "    # print(\"raw prediction: \", model.predict(single_seq_pair))\n",
        "    # model gives a one-hot encoded array\n",
        "    pred = model.predict(single_seq_pair)[0]\n",
        "    # turns into label encoded array (word_id's)\n",
        "    pred_le = [np.argmax(oneHot_vec) for oneHot_vec in pred]\n",
        "    # print(\"pred_le: \", pred_le)\n",
        "    pred_tokens = []\n",
        "    for id in pred_le:\n",
        "        try:\n",
        "            word = reverse_tgt_vocab_dict[id]\n",
        "            pred_tokens.append(word)\n",
        "        except KeyError:\n",
        "            break\n",
        "    return ' '.join(pred_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "5CO9MHfmQ2z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the 5th sentence in X_test\n",
        "for i in range(10):\n",
        "# ground truth sentences\n",
        "  print(\"actual source sentence: {}\".format([reverse_src_vocab_dict[id] for id in X_test[0][i] if id != 0]))\n",
        "  print(\"actual target sentence: {}\".format([reverse_tgt_vocab_dict[id] for id in X_test[1][i] if id != 0]))\n",
        "  print(\"predicted target sentence: {}\".format(pred_seq(eng_cn_translator, [X_test[0][i:i+1], X_test[1][i:i+1]], reverse_tgt_vocab_dict)))\n",
        "  print(\"-\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZJQpWOUclGb",
        "outputId": "211732f8-a95c-49be-ca57-89f7245484c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual source sentence: ['<start>', 'how', 'much', 'did', 'this', 'cost', '?', '<end>']\n",
            "actual target sentence: ['<start>', '多', '少', '錢', '？', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "predicted target sentence: <start> 多 少 錢 ？ <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'keep', 'the', 'dog', 'out', '.', '<end>']\n",
            "actual target sentence: ['<start>', '别', '让', '狗', '进', '来', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "predicted target sentence: <start> 别 让 狗 进 来 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'there', 'were', 'no', 'mistakes', '.', '<end>']\n",
            "actual target sentence: ['<start>', '没', '有', '错', '误', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "predicted target sentence: <start> 没 有 错 误 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'there', 'isn', \"'t\", 'any', 'soap', '.', '<end>']\n",
            "actual target sentence: ['<start>', '沒', '有', '任', '何', '肥', '皂', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "predicted target sentence: <start> 沒 有 任 何 肥 通 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'where', 'is', 'the', 'elevator', '?', '<end>']\n",
            "actual target sentence: ['<start>', '電', '梯', '在', '哪', '裡', '？', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "predicted target sentence: <start> 電 梯 在 哪 裡 ？ <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'don', \"'t\", 'tell', 'anyone', '.', '<end>']\n",
            "actual target sentence: ['<start>', '对', '谁', '都', '别', '说', '哟', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "predicted target sentence: <start> 对 谁 都 别 说 哟 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'he', 'likes', 'playing', 'soccer', '.', '<end>']\n",
            "actual target sentence: ['<start>', '他', '喜', '歡', '踢', '足', '球', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "predicted target sentence: <start> 他 喜 歡 踢 足 球 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'tom', 'is', 'now', 'on', 'vacation', '.', '<end>']\n",
            "actual target sentence: ['<start>', '汤', '姆', '正', '在', '度', '假', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "predicted target sentence: <start> 汤 姆 正 在 度 假 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'i', 'have', 'to', 'go', 'to', 'sleep', '.', '<end>']\n",
            "actual target sentence: ['<start>', '我', '必', '须', '睡', '了', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "predicted target sentence: <start> 我 必 须 睡 了 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "actual source sentence: ['<start>', 'she', 'asked', 'me', 'a', 'question', '.', '<end>']\n",
            "actual target sentence: ['<start>', '她', '問', '了', '我', '一', '個', '問', '題', '。', '<end>']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "predicted target sentence: <start> 她 問 了 我 一 個 問 題 。 <end>\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use BLEU score to evaluate model\n"
      ],
      "metadata": {
        "id": "rPDLHzYiOkKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "HYUrDwvCRQDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0][1:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LYhjWAtR4aS",
        "outputId": "83a890db-d4f3-4d27-be00-cdbda2feaf47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1, 236, 402,   7,  14,   5,   2,   0,   0,   0,   0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_NMT(model, X_input, seq_pairs_input, reverse_tgt_vocab_dict):\n",
        "    \"\"\"\n",
        "    Evaluates trained NMT model on a given dataset\n",
        "    ------------------------------------------------\n",
        "    X_input:\n",
        "        [a few enc_inputs, a few dec_inputs]\n",
        "        date type: numpy array of shape: [(n_sentences, src_max_seq_length), (n_sentences, tgt_max_seq_length)]\n",
        "    seq_pairs_input:\n",
        "        source and target sentences\n",
        "        data type: list of list of strings\n",
        "    \"\"\"\n",
        "    # Step 0: Check shape and specify max_seq_length\n",
        "    print(\"shape of src_seqs: [{}, {}]\".format(X_input[0].shape, X_input[1].shape)) # [(8000, 13), (8000, 22)]\n",
        "    true, predicted = [], []\n",
        "\n",
        "    src_max_seq_length = X_input[0].shape[1]  #11\n",
        "    tgt_max_seq_length = X_input[1].shape[1]  #22\n",
        "\n",
        "    # Step 1: Translate each sentence\n",
        "    for i in range(10): # 8000\n",
        "        # Step 2: Prepare training data of one sample (current sentence)\n",
        "        single_seq_pair = [X_input[0][i:i+1], X_input[1][i:i+1]]   #(英文,中文) (data,label)\n",
        "        # src_seq shape: [(?, 11), (?, 22)]\n",
        "        # Step 3: Predict a single sample and creates a string of tokens\n",
        "        translated_sentence = pred_seq(model, single_seq_pair, reverse_tgt_vocab_dict)\n",
        "\n",
        "\n",
        "        # Step 4: Collect ground truth sentences and predicted sentences\n",
        "        src_sentence = [reverse_src_vocab_dict[id] for id in X_input[0][i] if id != 0]\n",
        "        tgt_sentence = [reverse_tgt_vocab_dict[id] for id in X_input[1][i] if id != 0]\n",
        "\n",
        "        # lists translation results of first five sentences\n",
        "        if i < 5:\n",
        "            print(\"source: {}\\ntarget: {}\\ntranslated: {}\".format(src_sentence, tgt_sentence, translated_sentence))\n",
        "\n",
        "        true.append([tgt_sentence])  #label     # 因為參考翻譯可以有多個版本，所以要加一層外部列表\n",
        "        predicted.append(translated_sentence.split())  #predict\n",
        "    #print('true:',true)\n",
        "    #print('predict:',predicted)\n",
        "\n",
        "    # Step 5: Calculate corpus BLEU scores on the dataset X_input\n",
        "    ## Individual n-gram scores\n",
        "\n",
        "    print(\"Individual 1-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (1, 0, 0, 0))))\n",
        "    print(\"Individual 2-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (0, 1, 0, 0))))\n",
        "    print(\"Individual 3-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (1, 1, 1, 0))))\n",
        "    print(\"Individual 4-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (1, 0, 0, 1))))\n",
        "\n",
        "    ## Cumulative n-gram scores\n",
        "    print(\"Cumulative 1-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (1, 0, 0, 0))))\n",
        "    print(\"Cumulative 2-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (.5, .5, 0, 0))))\n",
        "    print(\"Cumulative 3-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (.33, .33, .33, 0))))\n",
        "    print(\"Cumulative 4-gram score: {:.6f}\".format(corpus_bleu(true, predicted, weights = (.25, .25, .25, .25))))\n",
        "\n",
        "\n",
        "    bleu_score = corpus_bleu(true, predicted)\n",
        "    print(\"Corpus BLEU score: {:.6f}\".format(bleu_score))\n",
        "\n",
        "# evaluate model on training dataset\n",
        "eval_NMT(eng_cn_translator, X_test, seq_pairs, reverse_tgt_vocab_dict)\n",
        "\n",
        "\n",
        "# evaluate model on training dataset\n",
        "#eval_NMT(eng_cn_translator, X_test, seq_pairs, reverse_tgt_vocab_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikZOzYjXOxZ4",
        "outputId": "b7e44610-8161-4a51-d5ef-5e90d54cbec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of src_seqs: [(2000, 11), (2000, 22)]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "source: ['<start>', 'how', 'much', 'did', 'this', 'cost', '?', '<end>']\n",
            "target: ['<start>', '多', '少', '錢', '？', '<end>']\n",
            "translated: <start> 多 少 錢 ？ <end>\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "source: ['<start>', 'keep', 'the', 'dog', 'out', '.', '<end>']\n",
            "target: ['<start>', '别', '让', '狗', '进', '来', '。', '<end>']\n",
            "translated: <start> 别 让 狗 进 来 。 <end>\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "source: ['<start>', 'there', 'were', 'no', 'mistakes', '.', '<end>']\n",
            "target: ['<start>', '没', '有', '错', '误', '。', '<end>']\n",
            "translated: <start> 没 有 错 误 。 <end>\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "source: ['<start>', 'there', 'isn', \"'t\", 'any', 'soap', '.', '<end>']\n",
            "target: ['<start>', '沒', '有', '任', '何', '肥', '皂', '。', '<end>']\n",
            "translated: <start> 沒 有 任 何 肥 通 。 <end>\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "source: ['<start>', 'where', 'is', 'the', 'elevator', '?', '<end>']\n",
            "target: ['<start>', '電', '梯', '在', '哪', '裡', '？', '<end>']\n",
            "translated: <start> 電 梯 在 哪 裡 ？ <end>\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Individual 1-gram score: 0.988095\n",
            "Individual 2-gram score: 0.972973\n",
            "Individual 3-gram score: 0.916325\n",
            "Individual 4-gram score: 0.933201\n",
            "Cumulative 1-gram score: 0.988095\n",
            "Cumulative 2-gram score: 0.980505\n",
            "Cumulative 3-gram score: 0.971575\n",
            "Cumulative 4-gram score: 0.964509\n",
            "Corpus BLEU score: 0.964509\n"
          ]
        }
      ]
    }
  ]
}